{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A0EWg4B6JVQjcC6MAjrJ90sujopNhr47",
      "authorship_tag": "ABX9TyOhVwsJ/hrwHqR395wT4noh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikumara666/Ravikumara666/blob/main/Adobe_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def load_csv_pairs(input_csv_paths, output_csv_paths):\n",
        "    input_data = []\n",
        "    output_data = []\n",
        "\n",
        "    for input_csv, output_csv in zip(input_csv_paths, output_csv_paths):\n",
        "        input_df = pd.read_csv(input_csv, header=None)\n",
        "        output_df = pd.read_csv(output_csv, header=None)\n",
        "\n",
        "        # Assuming each CSV contains polylines and their corresponding shapes\n",
        "        input_data.append(input_df.values)\n",
        "        output_data.append(output_df.values)\n",
        "\n",
        "    return input_data, output_data\n",
        "\n",
        "def flatten_and_pad(data, fixed_size):\n",
        "    flattened_data = []\n",
        "    for shape_list in data:\n",
        "        sample_data = []\n",
        "        for shape in shape_list:\n",
        "            flattened_shape = shape.flatten()\n",
        "            if len(flattened_shape) > fixed_size:\n",
        "                flattened_shape = flattened_shape[:fixed_size]\n",
        "            elif len(flattened_shape) < fixed_size:\n",
        "                flattened_shape = np.pad(flattened_shape, (0, fixed_size - len(flattened_shape)), mode='constant', constant_values=0)\n",
        "            sample_data.append(flattened_shape)\n",
        "        flattened_data.append(np.concatenate(sample_data))\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "def preprocess_data(input_data, output_data, fixed_size=1000000):\n",
        "    standardized_input_data = flatten_and_pad(input_data, fixed_size)\n",
        "    standardized_output_data = flatten_and_pad(output_data, fixed_size)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    X = np.array(standardized_input_data)\n",
        "    y = np.array(standardized_output_data)\n",
        "\n",
        "    # Ensure that the shapes are consistent\n",
        "    assert X.shape[0] == y.shape[0], \"Mismatch in number of samples\"\n",
        "    assert X.shape[1] == y.shape[1], \"Mismatch in input data shape\"\n",
        "    assert y.shape[1] == X.shape[1], \"Mismatch in output data shape\"\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def save_predictions(predictions, file_path):\n",
        "    # Convert predictions to a DataFrame and save to CSV\n",
        "    pd.DataFrame(predictions).to_csv(file_path, header=False, index=False)\n",
        "\n",
        "def visualize_results(X_test, y_test, y_pred, num_samples=5, output_png_dir='output_png'):\n",
        "    if not os.path.exists(output_png_dir):\n",
        "        os.makedirs(output_png_dir)\n",
        "\n",
        "    for i in range(min(num_samples, len(X_test))):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Reshape to original form for visualization if necessary\n",
        "        X_test_shape = X_test[i].reshape(-1, 2)  # Adjust if needed\n",
        "        y_test_shape = y_test[i].reshape(-1, 2)  # Adjust if needed\n",
        "        y_pred_shape = y_pred[i].reshape(-1, 2)  # Adjust if needed\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title('Irregular Shape')\n",
        "        plt.scatter(X_test_shape[:, 0], X_test_shape[:, 1], s=1)\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title('Corrected Shape')\n",
        "        plt.scatter(y_test_shape[:, 0], y_test_shape[:, 1], s=1)\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title('Predicted Shape')\n",
        "        plt.scatter(y_pred_shape[:, 0], y_pred_shape[:, 1], s=1)\n",
        "\n",
        "        plt.savefig(os.path.join(output_png_dir, f'result_{i}.png'))\n",
        "        plt.close()\n",
        "\n",
        "def predict_and_save_new_data(model, scaler_X, scaler_y, new_input_csv_path, output_png_dir='output_png', fixed_size=1000000):\n",
        "    # Load and preprocess new data\n",
        "    new_input_data = pd.read_csv(new_input_csv_path, header=None).values\n",
        "    preprocessed_input = flatten_and_pad([new_input_data], fixed_size)[0]\n",
        "\n",
        "    # Standardize the new data\n",
        "    scaled_input = scaler_X.transform([preprocessed_input])\n",
        "\n",
        "    # Predict\n",
        "    scaled_predictions = model.predict(scaled_input)\n",
        "\n",
        "    # Inverse transform to original scale\n",
        "    predictions = scaler_y.inverse_transform(scaled_predictions)\n",
        "\n",
        "    # Reshape predictions for visualization\n",
        "    # Assuming predictions need to be reshaped back into original 2D shape\n",
        "    num_points = len(predictions[0]) // 2\n",
        "    reshaped_predictions = predictions[0].reshape(num_points, 2)\n",
        "\n",
        "    # Save as PNG\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(reshaped_predictions[:, 0], reshaped_predictions[:, 1], s=1)\n",
        "    plt.title('Predicted Shape')\n",
        "    plt.savefig(os.path.join(output_png_dir, 'new_data_predictions.png'))\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Example CSV file paths for training\n",
        "    input_csv_files = [\n",
        "        '/content/frag1.csv',\n",
        "        '/content/frag2.csv',\n",
        "        '/content/occlusion1.csv',\n",
        "        '/content/isolated.csv'\n",
        "    ]\n",
        "\n",
        "    output_csv_files = [\n",
        "        '/content/frag01_sol.csv',\n",
        "        '/content/frag2_sol.csv',\n",
        "        '/content/occlusion1_sol.csv',\n",
        "        '/content/isolated_sol.csv'\n",
        "    ]\n",
        "\n",
        "    input_data, output_data = load_csv_pairs(input_csv_files, output_csv_files)\n",
        "    X, y = preprocess_data(input_data, output_data)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train = scaler_X.fit_transform(X_train)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "    y_train = scaler_y.fit_transform(y_train)\n",
        "    y_test = scaler_y.transform(y_test)\n",
        "\n",
        "    # Train a Multi-layer Perceptron Regressor (MLP)\n",
        "    model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=200, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    with open('trained_model.pkl', 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)\n",
        "    with open('scaler_X.pkl', 'wb') as scaler_X_file:\n",
        "        pickle.dump(scaler_X, scaler_X_file)\n",
        "    with open('scaler_y.pkl', 'wb') as scaler_y_file:\n",
        "        pickle.dump(scaler_y, scaler_y_file)\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(X_test, y_test, y_pred)\n",
        "\n",
        "    # Predict and save results for new data\n",
        "    new_input_csv_path = '/content/frag0.csv'\n",
        "    predict_and_save_new_data(model, scaler_X, scaler_y, new_input_csv_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "BBGbZbVdqLSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}